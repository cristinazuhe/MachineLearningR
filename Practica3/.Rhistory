set.seed(1)
names(Boston)
boston_bagging = randomForest(medv~.,data=Boston, subset=indices_train,mtry=13,ntree=40,importance =TRUE)
predict.bag = predict (boston_bagging , Boston_test)
mean((predict.bag -Boston_test$medv)^2)
set.seed(1)
names(Boston)
boston_bagging = randomForest(medv~.,data=Boston, subset=indices_train,mtry=13,ntree=30,importance =TRUE)
predict.bag = predict (boston_bagging , Boston_test)
mean((predict.bag -Boston_test$medv)^2)
library(randomForest)
library(MASS)
set.seed(2)
indices_train = sample(1:nrow(Boston),8*nrow(Boston)%/%10)
Boston_train = Boston[indices_train,]
Boston_test = Boston[-indices_train,]
set.seed(2)
names(Boston)
boston_bagging = randomForest(medv~.,data=Boston, subset=indices_train,mtry=13,ntree=30,importance =TRUE)
predict.bag = predict (boston_bagging , Boston_test)
mean((predict.bag -Boston_test$medv)^2)
set.seed(2)
names(Boston)
boston_bagging = randomForest(medv~.,data=Boston, subset=indices_train,mtry=13,ntree=30,importance =TRUE)
predict.bag = predict (boston_bagging , Boston_test)
mean((predict.bag -Boston_test$medv)^2)
set.seed(2)
boston_rf= randomForest(medv~.,data=Boston , subset=indices_train , mtry=13/3,importance =TRUE)
predict.rf = predict(boston_rf , Boston_test)
mean((predict.rf-Boston_test$medv)^2)
set.seed(2)
boston_rf= randomForest(medv~.,data=Boston , subset=indices_train , mtry=13/3,,ntree=30, importance =TRUE)
predict.rf = predict(boston_rf , Boston_test)
mean((predict.rf-Boston_test$medv)^2)
set.seed(2)
boston_rf= randomForest(medv~.,data=Boston , subset=indices_train , mtry=13/3, ntree=30, importance =TRUE)
predict.rf = predict(boston_rf , Boston_test)
mean((predict.rf-Boston_test$medv)^2)
set.seed(2)
boston_rf= randomForest(medv~.,data=Boston , subset=indices_train , mtry=13/3, ntree=30, importance =TRUE)
predict.rf = predict(boston_rf , Boston_test)
mean((predict.rf-Boston_test$medv)^2)
library(randomForest)
library(MASS)
set.seed(2)
indices_train = sample(1:nrow(Boston),8*nrow(Boston)%/%10)
Boston_train = Boston[indices_train,]
Boston_test = Boston[-indices_train,]
set.seed(2)
names(Boston)
boston_bagging = randomForest(medv~.,data=Boston, subset=indices_train,mtry=13,ntree=30,importance =TRUE)
predict.bag = predict (boston_bagging , Boston_test)
mean((predict.bag -Boston_test$medv)^2)
set.seed(2)
boston_rf= randomForest(medv~.,data=Boston , subset=indices_train , mtry=13/3, ntree=30, importance =TRUE)
predict.rf = predict(boston_rf , Boston_test)
mean((predict.rf-Boston_test$medv)^2)
library(gbm)
set.seed(2)
boston_boosting = gbm(medv~.,data=Boston_train, distribution = "gaussian", n.trees = 6000, interaction.depth = 4)
predict.boost = predict(boston_boosting, Boston_test, n.trees = 6000)
mean((predict.boost-Boston_test$medv)^2)
library(gbm)
set.seed(2)
boston_boosting = gbm(medv~.,data=Boston_train, distribution = "gaussian", n.trees = 600, interaction.depth = 4)
predict.boost = predict(boston_boosting, Boston_test, n.trees = 600)
mean((predict.boost-Boston_test$medv)^2)
library(gbm)
set.seed(2)
boston_boosting = gbm(medv~.,data=Boston_train, distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
predict.boost = predict(boston_boosting, Boston_test, n.trees = 5000)
mean((predict.boost-Boston_test$medv)^2)
library(gbm)
set.seed(2)
boston_boosting = gbm(medv~.,data=Boston_train, distribution = "gaussian", n.trees = 5000, interaction.depth = 5)
predict.boost = predict(boston_boosting, Boston_test, n.trees = 5000)
mean((predict.boost-Boston_test$medv)^2)
set.seed(2)
boston_rf= randomForest(medv~.,data=Boston , subset=indices_train , mtry=13/3, ntree=30, importance =TRUE)
predict.rf = predict(boston_rf , Boston_test)
mean((predict.rf-Boston_test$medv)^2)
library(gbm)
set.seed(2)
boston_boosting = gbm(medv~.,data=Boston_train, distribution = "gaussian", n.trees = 5000, interaction.depth = 6)
predict.boost = predict(boston_boosting, Boston_test, n.trees = 5000)
mean((predict.boost-Boston_test$medv)^2)
library(gbm)
set.seed(2)
boston_boosting = gbm(medv~.,data=Boston_train, distribution = "gaussian", n.trees = 5000, interaction.depth = 7)
predict.boost = predict(boston_boosting, Boston_test, n.trees = 5000)
mean((predict.boost-Boston_test$medv)^2)
library(gbm)
set.seed(2)
boston_boosting = gbm(medv~.,data=Boston_train, distribution = "gaussian", n.trees = 4500, interaction.depth = 8)
predict.boost = predict(boston_boosting, Boston_test, n.trees = 4500)
mean((predict.boost-Boston_test$medv)^2)
library(gbm)
set.seed(2)
boston_boosting = gbm(medv~.,data=Boston_train, distribution = "gaussian", n.trees = 4000, interaction.depth = 8)
predict.boost = predict(boston_boosting, Boston_test, n.trees = 4000)
mean((predict.boost-Boston_test$medv)^2)
set.seed(2)
boston_rf= randomForest(medv~.,data=Boston , subset=indices_train , mtry=13/3, ntree=30, importance =TRUE)
predict.rf = predict(boston_rf , Boston_test)
mean((predict.rf-Boston_test$medv)^2)
set.seed(2)
boston_rf= randomForest(medv~.,data=Boston , subset=indices_train ,  ntree=30, importance =TRUE)
predict.rf = predict(boston_rf , Boston_test)
mean((predict.rf-Boston_test$medv)^2)
set.seed(2)
boston_rf= randomForest(medv~.,data=Boston , subset=indices_train , mtry=13/3, ntree=30, importance =TRUE)
predict.rf = predict(boston_rf , Boston_test)
mean((predict.rf-Boston_test$medv)^2)
set.seed(2)
boston_rf= randomForest(medv~.,data=Boston , subset=indices_train , mtry=13/2, ntree=30, importance =TRUE)
predict.rf = predict(boston_rf , Boston_test)
mean((predict.rf-Boston_test$medv)^2)
set.seed(2)
names(Boston)
boston_bagging = randomForest(medv~.,data=Boston, subset=indices_train,mtry=13,ntree=30,importance =TRUE)
predict.bag = predict (boston_bagging , Boston_test)
mean((predict.bag -Boston_test$medv)^2)
set.seed(2)
boston_rf= randomForest(medv~.,data=Boston , subset=indices_train , mtry=13/3, ntree=30, importance =TRUE)
predict.rf = predict(boston_rf , Boston_test)
mean((predict.rf-Boston_test$medv)^2)
library(gbm)
set.seed(2)
boston_boosting = gbm(medv~.,data=Boston_train, distribution = "gaussian", n.trees = 4000, interaction.depth = 8)
predict.boost = predict(boston_boosting, Boston_test, n.trees = 4000)
mean((predict.boost-Boston_test$medv)^2)
library(ISLR)
Predictoras = cbind(Auto$mpg, Auto$displacement, Auto$horsepower, Auto$weight, Auto$year)
colnames(Predictoras) = c("mpg", "displacement", "horsepower", "weight", "year")
set.seed(2)
indices_test = sample(nrow(Predictoras), 2*nrow(Predictoras)%/%10, replace=FALSE)
Predictoras_test = Predictoras[indices_test,]
Predictoras_train = Predictoras[-indices_test,]
mediana_mpg = median(Auto$mpg)
Predictoras_train_mpg01 = data.frame(Predictoras_train,
mpg01=sign(Predictoras_train[,"mpg"] - mediana_mpg))
Predictoras_test_mpg01 = data.frame(Predictoras_test,
mpg01=sign(Predictoras_test[,"mpg"] - mediana_mpg))
for(i in 1:nrow(Predictoras_test_mpg01)){
if((Predictoras_test_mpg01[i,"mpg01"]) == -1)
Predictoras_test_mpg01[i,"mpg01"] = 0
}
for(i in 1:nrow(Predictoras_train_mpg01)){
if((Predictoras_train_mpg01[i,"mpg01"]) == -1)
Predictoras_train_mpg01[i,"mpg01"] = 0
}
ajuste_rlog <- glm(mpg01 ~ displacement + horsepower + weight + year,
data= Predictoras_train_mpg01, family=binomial)
probabilidades = predict(ajuste_rlog, Predictoras_test_mpg01, type="response")
probabilidades01 = rep(1,dim(Predictoras_test_mpg01)[1])
probabilidades01[probabilidades<0.5] = 0
matriz_confusion = table(probabilidades01, Predictoras_test_mpg01$mpg01)
print(matriz_confusion)
error = (matriz_confusion[1,2] + matriz_confusion[2,1])/sum(matriz_confusion)
print(error)
library(class)
library(e1071)
set.seed(2)
columnas=  dim(Predictoras_train_mpg01)[2]
datos_normalizados = scale(rbind(Predictoras_train_mpg01[,-columnas],
Predictoras_test_mpg01[,-columnas]))
test_normalizada = datos_normalizados[indices_test,]
train_normalizada = datos_normalizados[-indices_test,]
mpg01_train = Predictoras_train_mpg01[,columnas]
mpg01_test = Predictoras_test_mpg01[,columnas]
mpg01_ambos = c(mpg01_train, mpg01_test)
posibles_k <- tune.knn(datos_normalizados, as.factor(mpg01_ambos), k=1:10,
tune_control = tune.control(sampling = "cross"))
summary(posibles_k)
predicciones = knn(Predictoras_train, Predictoras_test, mpg01_train, k=5)
matriz_confusion_knn = table(predicciones, mpg01_test)
print(matriz_confusion_knn)
errorknn = ((matriz_confusion_knn[1,2] + matriz_confusion_knn[2,1])/sum(matriz_confusion_knn))
print(errorknn)
View(test_normalizada)
View(test_normalizada)
View(Predictoras_test_mpg01)
View(Predictoras_test_mpg01)
Boston
names(Boston)
Predictoras_train_mpg01
dim(Predictoras_train_mpg01)
dim(Predictoras_train_mpg01)[2]
Predictoras_train_mpg01[,-columnas]
datos_normalizados
View(train_normalizada)
View(train_normalizada)
View(Predictoras_train_mpg01)
View(Predictoras_train_mpg01)
dim(Predictoras_train_mpg01)[1]
columnas=  dim(Predictoras_train_mpg01)[2]
datos_normalizados = scale(rbind(Predictoras_train_mpg01[,-columnas],
Predictoras_test_mpg01[,-columnas]))
test_normalizada = datos_normalizados[1:dim(Predictoras_train_mpg01)[1],]
train_normalizada = datos_normalizados[dim(Predictoras_train_mpg01)[1] +1: dim(datos_normalizados)[1],]
datos_normalizados[1:dim(Predictoras_train_mpg01)[1],]
columnas=  dim(Predictoras_train_mpg01)[2]
datos_normalizados = scale(rbind(Predictoras_train_mpg01[,-columnas],
Predictoras_test_mpg01[,-columnas]))
test_normalizada = datos_normalizados[1:dim(Predictoras_train_mpg01)[1],]
train_normalizada = datos_normalizados[(dim(Predictoras_train_mpg01)[1] +1): dim(datos_normalizados)[1],]
mpg01_train = Predictoras_train_mpg01[,columnas]
mpg01_test = Predictoras_test_mpg01[,columnas]
mpg01_ambos = c(mpg01_train, mpg01_test)
posibles_k <- tune.knn(datos_normalizados, as.factor(mpg01_ambos), k=1:10,
tune_control = tune.control(sampling = "cross"))
summary(posibles_k)
predicciones = knn(Predictoras_train, Predictoras_test, mpg01_train, k=8)
matriz_confusion_knn = table(predicciones, mpg01_test)
print(matriz_confusion_knn)
errorknn = ((matriz_confusion_knn[1,2] + matriz_confusion_knn[2,1])/sum(matriz_confusion_knn))
print(errorknn)
library(ISLR)
library(ISLR)
Predictoras = cbind(Auto$mpg, Auto$displacement, Auto$horsepower, Auto$weight, Auto$year)
colnames(Predictoras) = c("mpg", "displacement", "horsepower", "weight", "year")
set.seed(2)
indices_test = sample(nrow(Predictoras), 2*nrow(Predictoras)%/%10, replace=FALSE)
Predictoras_test = Predictoras[indices_test,]
Predictoras_train = Predictoras[-indices_test,]
mediana_mpg = median(Auto$mpg)
Predictoras_train_mpg01 = data.frame(Predictoras_train,
mpg01=sign(Predictoras_train[,"mpg"] - mediana_mpg))
Predictoras_test_mpg01 = data.frame(Predictoras_test,
mpg01=sign(Predictoras_test[,"mpg"] - mediana_mpg))
for(i in 1:nrow(Predictoras_test_mpg01)){
if((Predictoras_test_mpg01[i,"mpg01"]) == -1)
Predictoras_test_mpg01[i,"mpg01"] = 0
}
for(i in 1:nrow(Predictoras_train_mpg01)){
if((Predictoras_train_mpg01[i,"mpg01"]) == -1)
Predictoras_train_mpg01[i,"mpg01"] = 0
}
ajuste_rlog <- glm(mpg01 ~ displacement + horsepower + weight + year,
data= Predictoras_train_mpg01, family=binomial)
probabilidades = predict(ajuste_rlog, Predictoras_test_mpg01, type="response")
probabilidades01 = rep(1,dim(Predictoras_test_mpg01)[1])
probabilidades01[probabilidades<0.5] = 0
matriz_confusion = table(probabilidades01, Predictoras_test_mpg01$mpg01)
print(matriz_confusion)
error = (matriz_confusion[1,2] + matriz_confusion[2,1])/sum(matriz_confusion)
print(error)
library(class)
library(e1071)
set.seed(2)
columnas=  dim(Predictoras_train_mpg01)[2]
datos_normalizados = scale(rbind(Predictoras_train_mpg01[,-columnas],
Predictoras_test_mpg01[,-columnas]))
test_normalizada = datos_normalizados[1:dim(Predictoras_train_mpg01)[1],]
train_normalizada = datos_normalizados[(dim(Predictoras_train_mpg01)[1] +1):
dim(datos_normalizados)[1],]
mpg01_train = Predictoras_train_mpg01[,columnas]
mpg01_test = Predictoras_test_mpg01[,columnas]
mpg01_ambos = c(mpg01_train, mpg01_test)
posibles_k <- tune.knn(datos_normalizados, as.factor(mpg01_ambos), k=1:10,
tune_control = tune.control(sampling = "cross"))
summary(posibles_k)
predicciones = knn(train_normalizada, test_normalizada, mpg01_train, k=8)
matriz_confusion_knn = table(predicciones, mpg01_test)
print(matriz_confusion_knn)
errorknn = ((matriz_confusion_knn[1,2] + matriz_confusion_knn[2,1])/sum(matriz_confusion_knn))
print(errorknn)
posibles_k <- tune.knn(datos_normalizados, as.factor(mpg01_ambos), k=1:10,
tune_control = tune.control(sampling = "cross"))
summary(posibles_k)
predicciones = knn(train_normalizada, test_normalizada, mpg01_train, k=5)
matriz_confusion_knn = table(predicciones, mpg01_test)
print(matriz_confusion_knn)
mpg01_train
train_normalizada
test_normalizada
pairs(Auto)
boxplot(Auto)
Predictoras = cbind(Auto$mpg, Auto$displacement, Auto$horsepower, Auto$weight, Auto$year)
colnames(Predictoras) = c("mpg", "displacement", "horsepower", "weight", "year")
set.seed(2)
indices_test = sample(nrow(Predictoras), 2*nrow(Predictoras)%/%10, replace=FALSE)
Predictoras_test = Predictoras[indices_test,]
Predictoras_train = Predictoras[-indices_test,]
mediana_mpg = median(Auto$mpg)
Predictoras_train_mpg01 = data.frame(Predictoras_train,
mpg01=sign(Predictoras_train[,"mpg"] - mediana_mpg))
Predictoras_test_mpg01 = data.frame(Predictoras_test,
mpg01=sign(Predictoras_test[,"mpg"] - mediana_mpg))
for(i in 1:nrow(Predictoras_test_mpg01)){
if((Predictoras_test_mpg01[i,"mpg01"]) == -1)
Predictoras_test_mpg01[i,"mpg01"] = 0
}
for(i in 1:nrow(Predictoras_train_mpg01)){
if((Predictoras_train_mpg01[i,"mpg01"]) == -1)
Predictoras_train_mpg01[i,"mpg01"] = 0
}
ajuste_rlog <- glm(mpg01 ~ displacement + horsepower + weight + year,
data= Predictoras_train_mpg01, family=binomial)
probabilidades = predict(ajuste_rlog, Predictoras_test_mpg01, type="response")
probabilidades01 = rep(1,dim(Predictoras_test_mpg01)[1])
probabilidades01[probabilidades<0.5] = 0
matriz_confusion = table(probabilidades01, Predictoras_test_mpg01$mpg01)
print(matriz_confusion)
error = (matriz_confusion[1,2] + matriz_confusion[2,1])/sum(matriz_confusion)
print(error)
library(class)
library(e1071)
set.seed(2)
columnas=  dim(Predictoras_train_mpg01)[2]
datos_normalizados = scale(rbind(Predictoras_train_mpg01[,-columnas],
Predictoras_test_mpg01[,-columnas]))
test_normalizada = datos_normalizados[1:dim(Predictoras_train_mpg01)[1],]
train_normalizada = datos_normalizados[(dim(Predictoras_train_mpg01)[1] +1):
dim(datos_normalizados)[1],]
test_normalizada
train_normalizada
columnas=  dim(Predictoras_train_mpg01)[2]
datos_normalizados = scale(rbind(Predictoras_test_mpg01[,-columnas],
Predictoras_train_mpg01[,-columnas]))
test_normalizada = datos_normalizados[1:dim(Predictoras_test_mpg01)[1],]
train_normalizada = datos_normalizados[(dim(Predictoras_test_mpg01)[1] +1):
dim(datos_normalizados)[1],]
test_normalizada
train_normalizada
dim(Predictoras_test_mpg01)[1] +1
(dim(Predictoras_test_mpg01)[1] +1):
dim(datos_normalizados)[1]
train_normalizada
mpg01_train = Predictoras_train_mpg01[,columnas]
mpg01_test = Predictoras_test_mpg01[,columnas]
mpg01_ambos = c(mpg01_train, mpg01_test)
posibles_k <- tune.knn(datos_normalizados, as.factor(mpg01_ambos), k=1:10,
tune_control = tune.control(sampling = "cross"))
summary(posibles_k)
predicciones = knn(train_normalizada, test_normalizada, mpg01_train, k=4)
matriz_confusion_knn = table(predicciones, mpg01_test)
print(matriz_confusion_knn)
errorknn = ((matriz_confusion_knn[1,2] + matriz_confusion_knn[2,1])/sum(matriz_confusion_knn))
print(errorknn)
predicciones = knn(train_normalizada, test_normalizada, mpg01_train, k=4)
matriz_confusion_knn = table(predicciones, mpg01_test)
print(matriz_confusion_knn)
errorknn = ((matriz_confusion_knn[1,2] + matriz_confusion_knn[2,1])/sum(matriz_confusion_knn))
print(errorknn)
matriz_confusion = table(probabilidades01, Predictoras_test_mpg01$mpg01)
print(matriz_confusion)
error = (matriz_confusion[1,2] + matriz_confusion[2,1])/sum(matriz_confusion)
print(error)
predicciones = knn(train_normalizada, test_normalizada, mpg01_train, k=4)
matriz_confusion_knn = table(predicciones, mpg01_test)
print(matriz_confusion_knn)
errorknn = ((matriz_confusion_knn[1,2] + matriz_confusion_knn[2,1])/sum(matriz_confusion_knn))
print(errorknn)
#Cargamos la BD Auto
library(ISLR)
attach(Auto)
f = nrow(Auto)
a = round(f*0.8)
train.index=sample(f, a, replace=FALSE)
train = Auto[train.index, ]
test = Auto[-train.index, ]
mpg01 = rep(0, nrow(Auto))
mpg01[mpg > median(mpg)] = 1
Auto2 = data.frame(Auto, mpg01)
Datos.train = Auto2[train.index, ]
Datos.test = Auto2[-train.index, ]
mpg01.test = mpg01[-train.index]
modelo = glm(mpg01~cylinders+weight+displacement+horsepower, data=Auto2, family="binomial", subset=train.index)
pr = predict(modelo, Datos.test, type="response")
prediccion.glm = rep(-1, length(pr))
prediccion.glm[pr > 0.5] = 1
mpg_11 = rep(-1, nrow(Auto))
mpg_11[mpg > median(mpg)] = 1
mpg_11.test = mpg_11[-train.index]
table(prediccion.glm, mpg_11.test)
mean(prediccion.glm != mpg_11.test)
library(class)
train.knn = cbind(cylinders, weight, displacement, horsepower)[train.index, ]
test.knn = cbind(cylinders, weight, displacement, horsepower)[-train.index, ]
mpg01.train = mpg01[train.index]
vector.knn = vector()
for (k in 1:100){
modelo = knn(train.knn, test.knn, mpg01.train, k=k)
vector.knn = append(vector.knn,mean(modelo != mpg01.test))
}
M = matrix(vector.knn,nrow=1,ncol=length(vector.knn),dimnames=list(c("Acierto"), paste("K=", c(1:length(vector.knn)))))
k.optimo = which.min(M)
paste("K test = ",k.optimo)
modelo = knn(train.knn, test.knn, mpg01.train, k=k.optimo)
table(modelo, mpg01.test)
mean(modelo != mpg01.test)
library(ROCR)
rocplot=function(pred, truth, ...){
predob = prediction (pred, truth)
perf = performance (predob, "tpr", "fpr")
plot(perf, ...)
}
rocplot(prediccion.glm, mpg_11.test)
rocplot(as.numeric(modelo), mpg01.test)
library(ROCR)
rocplot=function(pred, truth, ...){
predob = prediction (pred, truth)
perf = performance (predob, "tpr", "fpr")
plot(perf, ...)
}
rocplot(probabilidades01, Predictoras_test_mpg01$mpg01)
rocplot(as.numeric(predicciones), mpg01_test)
library(ROCR)
rocplot=function(pred, truth, ...){
predob = prediction (pred, truth)
perf = performance (predob, "tpr", "fpr")
plot(perf, ...)
}
#rocplot(probabilidades01, Predictoras_test_mpg01$mpg01)
#rocplot(as.numeric(predicciones), mpg01_test)
library(ROCR)
rocplot=function(pred, truth, ...){
predob = prediction (pred, truth)
perf = performance (predob, "tpr", "fpr")
plot(perf, ...)
}
rocplot(probabilidades01, Predictoras_test_mpg01$mpg01)
#rocplot(as.numeric(predicciones), mpg01_test)
Predictoras = cbind(Auto$mpg, Auto$displacement, Auto$horsepower, Auto$weight, Auto$year)
colnames(Predictoras) = c("mpg", "displacement", "horsepower", "weight", "year")
set.seed(2)
indices_test = sample(nrow(Predictoras), 2*nrow(Predictoras)%/%10, replace=FALSE)
Predictoras_test = Predictoras[indices_test,]
Predictoras_train = Predictoras[-indices_test,]
mediana_mpg = median(Auto$mpg)
Predictoras_train_mpg01 = data.frame(Predictoras_train,
mpg01=sign(Predictoras_train[,"mpg"] - mediana_mpg))
Predictoras_test_mpg01 = data.frame(Predictoras_test,
mpg01=sign(Predictoras_test[,"mpg"] - mediana_mpg))
for(i in 1:nrow(Predictoras_test_mpg01)){
if((Predictoras_test_mpg01[i,"mpg01"]) == -1)
Predictoras_test_mpg01[i,"mpg01"] = 0
}
for(i in 1:nrow(Predictoras_train_mpg01)){
if((Predictoras_train_mpg01[i,"mpg01"]) == -1)
Predictoras_train_mpg01[i,"mpg01"] = 0
}
ajuste_rlog <- glm(mpg01 ~ displacement + horsepower + weight + year,
data= Predictoras_train_mpg01, family=binomial)
probabilidades = predict(ajuste_rlog, Predictoras_test_mpg01, type="response")
probabilidades01 = rep(1,dim(Predictoras_test_mpg01)[1])
probabilidades01[probabilidades<0.5] = 0
matriz_confusion = table(probabilidades01, Predictoras_test_mpg01$mpg01)
print(matriz_confusion)
error = (matriz_confusion[1,2] + matriz_confusion[2,1])/sum(matriz_confusion)
print(error)
library(class)
library(e1071)
set.seed(2)
columnas=  dim(Predictoras_train_mpg01)[2]
datos_normalizados = scale(rbind(Predictoras_test_mpg01[,-columnas],
Predictoras_train_mpg01[,-columnas]))
test_normalizada = datos_normalizados[1:dim(Predictoras_test_mpg01)[1],]
train_normalizada = datos_normalizados[(dim(Predictoras_test_mpg01)[1] +1):
dim(datos_normalizados)[1],]
mpg01_train = Predictoras_train_mpg01[,columnas]
mpg01_test = Predictoras_test_mpg01[,columnas]
mpg01_ambos = c(mpg01_train, mpg01_test)
posibles_k <- tune.knn(datos_normalizados, as.factor(mpg01_ambos), k=1:10,
tune_control = tune.control(sampling = "cross"))
summary(posibles_k)
predicciones = knn(train_normalizada, test_normalizada, mpg01_train, k=4)
matriz_confusion_knn = table(predicciones, mpg01_test)
print(matriz_confusion_knn)
errorknn = ((matriz_confusion_knn[1,2] + matriz_confusion_knn[2,1])/sum(matriz_confusion_knn))
print(errorknn)
library(ROCR)
rocplot=function(pred, truth, ...){
predob = prediction (pred, truth)
perf = performance (predob, "tpr", "fpr")
plot(perf, ...)
}
rocplot(probabilidades01, Predictoras_test_mpg01$mpg01)
#rocplot(as.numeric(predicciones), mpg01_test)
library(ROCR)
rocplot=function(pred, truth, ...){
predob = prediction (pred, truth)
perf = performance (predob, "tpr", "fpr")
plot(perf, ...)
}
rocplot(probabilidades01, Predictoras_test_mpg01$mpg01)
rocplot(as.numeric(predicciones), mpg01_test)
library(ROCR)
rocplot=function(pred, truth, ...){
predob = prediction (pred, truth)
perf = performance (predob, "tpr", "fpr")
plot(perf, ...)
}
rocplot(probabilidades01, Predictoras_test_mpg01$mpg01)
rocplot(predicciones, mpg01_test)
library(ROCR)
rocplot=function(pred, truth, ...){
predob = prediction (pred, truth)
perf = performance (predob, "tpr", "fpr")
plot(perf, ...)
}
rocplot(probabilidades01, Predictoras_test_mpg01$mpg01)
rocplot(as.numeric(predicciones), mpg01_test)
library(ROCR)
#rocplot=function(pred, truth, ...){
#  predob = prediction (pred, truth)
#  perf = performance (predob, "tpr", "fpr")
#  plot(perf, ...)
#}
rocplot(probabilidades01, Predictoras_test_mpg01$mpg01)
rocplot(as.numeric(predicciones), mpg01_test)
